{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional GANs 深度卷积GANs\n",
    "\n",
    "In this notebook, you'll build a GAN using convolutional layers in the generator and discriminator. This is called a Deep Convolutional GAN, or DCGAN for short. The DCGAN architecture was first explored last year and has seen impressive results in generating new images, you can read the [original paper here](https://arxiv.org/pdf/1511.06434.pdf).\n",
    "\n",
    "在这款笔记本中，您将使用生成器和鉴别器中的卷积层构建GAN。这被称为深卷积GAN，简称为DCGAN。 DCGAN架构去年首次被探索，并且在生成新图像方面取得了令人印象深刻的成果，您可以阅读 [原始论文](https://arxiv.org/pdf/1511.06434.pdf)。\n",
    "\n",
    "You'll be training DCGAN on the [Street View House Numbers](http://ufldl.stanford.edu/housenumbers/) (SVHN) dataset. These are color images of house numbers collected from Google street view. SVHN images are in color and much more variable than MNIST.\n",
    "\n",
    "你将在[Street View House Numbers](http://ufldl.stanford.edu/housenumbers/)（SVHN）数据集上训练DCGAN。这些是从Google街景视图收集的房屋号码的彩色图像。 SVHN图像是颜色，比MNIST更加变化。\n",
    "\n",
    "![SVHN Examples](assets/SVHN_examples.png)\n",
    "\n",
    "So, we'll need a deeper and more powerful network. This is accomplished through using convolutional layers in the discriminator and generator. It's also necessary to use batch normalization to get the convolutional networks to train. The only real changes compared to what [you saw previously](https://github.com/udacity/deep-learning/tree/master/gan_mnist) are in the generator and discriminator, otherwise the rest of the implementation is the same.\n",
    "\n",
    "所以我们需要一个更深入，更强大的网络。这通过在鉴别器和发生器中使用卷积层来实现。还需要使用批归一化来获得卷积网络的训练。与[you saw previously](https://github.com/udacity/deep-learning/tree/master/gan_mnist) 相比，唯一真正的变化是在生成器和鉴别器中，否则其余的实现是相同的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data 获得数据\n",
    "\n",
    "Here you can download the SVHN dataset. Run the cell above and it'll download to your machine.\n",
    "\n",
    "在这里可以下载SVHN数据集。 运行上面的单元格，它将下载到您的计算机。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_dir = 'data/'\n",
    "\n",
    "if not isdir(data_dir):\n",
    "    raise Exception(\"Data directory doesn't exist!\")\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(data_dir + \"train_32x32.mat\"):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN Training Set') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',\n",
    "            data_dir + 'train_32x32.mat',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isfile(data_dir + \"test_32x32.mat\"):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN Training Set') as pbar:\n",
    "        urlretrieve(\n",
    "            'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',\n",
    "            data_dir + 'test_32x32.mat',\n",
    "            pbar.hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These SVHN files are `.mat` files typically used with Matlab. However, we can load them in with `scipy.io.loadmat` which we imported above.\n",
    "\n",
    "这些SVHN文件是通常与Matlab一起使用的`.mat`文件。 但是，我们可以使用上面导入的 `scipy.io.loadmat` 加载它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = loadmat(data_dir + 'train_32x32.mat')\n",
    "testset = loadmat(data_dir + 'test_32x32.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I'm showing a small sample of the images. Each of these is 32x32 with 3 color channels (RGB). These are the real images we'll pass to the discriminator and what the generator will eventually fake.\n",
    "\n",
    "在这里，我展示了一小部分图像。 这些都是32x32，有3个彩色通道(RGB)。 这些是我们将传递给鉴别器的真实图像，以及发生器将最终假定的真实图像。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, trainset['X'].shape[3], size=36)\n",
    "fig, axes = plt.subplots(6, 6, sharex=True, sharey=True, figsize=(5,5),)\n",
    "for ii, ax in zip(idx, axes.flatten()):\n",
    "    ax.imshow(trainset['X'][:,:,:,ii], aspect='equal')\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we need to do a bit of preprocessing and getting the images into a form where we can pass batches to the network. First off, we need to rescale the images to a range of -1 to 1, since the output of our generator is also in that range. We also have a set of test and validation images which could be used if we're trying to identify the numbers in the images.\n",
    "\n",
    "在这里，我们需要进行一些预处理，并将图像转换成可以将批次传递到网络的形式。 首先，我们需要将图像重新缩放到-1到1的范围，因为我们的发生器的输出也在该范围内。 我们还有一组测试和验证图像，如果我们试图识别图像中的数字，可以使用它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scale(x, feature_range=(-1, 1)):\n",
    "    # scale to (0, 1)\n",
    "    x = ((x - x.min())/(255 - x.min()))\n",
    "    \n",
    "    # scale to feature_range\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, train, test, val_frac=0.5, shuffle=False, scale_func=None):\n",
    "        split_idx = int(len(test['y'])*(1 - val_frac))\n",
    "        self.test_x, self.valid_x = test['X'][:,:,:,:split_idx], test['X'][:,:,:,split_idx:]\n",
    "        self.test_y, self.valid_y = test['y'][:split_idx], test['y'][split_idx:]\n",
    "        self.train_x, self.train_y = train['X'], train['y']\n",
    "        \n",
    "        self.train_x = np.rollaxis(self.train_x, 3)\n",
    "        self.valid_x = np.rollaxis(self.valid_x, 3)\n",
    "        self.test_x = np.rollaxis(self.test_x, 3)\n",
    "        \n",
    "        if scale_func is None:\n",
    "            self.scaler = scale\n",
    "        else:\n",
    "            self.scaler = scale_func\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "    def batches(self, batch_size):\n",
    "        if self.shuffle:\n",
    "            idx = np.arange(len(dataset.train_x))\n",
    "            np.random.shuffle(idx)\n",
    "            self.train_x = self.train_x[idx]\n",
    "            self.train_y = self.train_y[idx]\n",
    "        \n",
    "        n_batches = len(self.train_y)//batch_size\n",
    "        for ii in range(0, len(self.train_y), batch_size):\n",
    "            x = self.train_x[ii:ii+batch_size]\n",
    "            y = self.train_y[ii:ii+batch_size]\n",
    "            \n",
    "            yield self.scaler(x), self.scaler(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Inputs 网络输入\n",
    "\n",
    "Here, just creating some placeholders like normal. 在这里，只是创建一些像正常的占位符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='input_real')\n",
    "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "    \n",
    "    return inputs_real, inputs_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator 生成器\n",
    "\n",
    "Here you'll build the generator network. The input will be our noise vector `z` as before. Also as before, the output will be a $tanh$ output, but this time with size 32x32 which is the size of our SVHN images.\n",
    "\n",
    "在这里，您将构建生成器网络。 输入将像以前一样是我们的噪声矢量 `z`。 同样如前所述，输出将是 $tanh$ 输出，但是这次大小为32x32，这是我们的SVHN图像的大小。\n",
    "\n",
    "What's new here is we'll use convolutional layers to create our new images. The first layer is a fully connected layer which is reshaped into a deep and narrow layer, something like 4x4x1024 as in the original DCGAN paper. Then we use batch normalization and a leaky ReLU activation. Next is a transposed convolution where typically you'd halve the depth and double the width and height of the previous layer. Again, we use batch normalization and leaky ReLU. For each of these layers, the general scheme is convolution > batch norm > leaky ReLU.\n",
    "\n",
    "这里有新功能，我们将使用卷积层来创建我们的新图像。 第一层是完全连接的层，其重新形成深而窄的层，像原始DCGAN纸中的4×4×1024。 然后我们使用批归一化和leaky ReLU激活。 接下来是一个转置的卷积，通常你会将深度减半，并加倍上一层的宽度和高度。 再次，我们使用批归一化和leaky ReLU。 对于这些层中的每一层，一般方案是 卷积 > 批归一化 > leaky ReLU。\n",
    "\n",
    "You keep stack layers up like this until you get the final transposed convolution layer with shape 32x32x3. Below is the archicture used in the original DCGAN paper:\n",
    "\n",
    "你可以像这样保持堆叠层，直到得到32x32x3形状的最终转置的卷积层。 以下是原始DCGAN论文中使用的archicture：\n",
    "\n",
    "![DCGAN Generator](assets/dcgan.png)\n",
    "\n",
    "Note that the final layer here is 64x64x3, while for our SVHN dataset, we only want it to be 32x32x3.\n",
    "\n",
    "请注意，这里的最后一层是64x64x3，而对于我们的SVHN数据集，我们只希望它是32x32x3。\n",
    "\n",
    ">**Exercise:** Build the transposed convolutional network for the generator in the function below. Be sure to use leaky ReLUs on all the layers except for the last tanh layer, as well as batch normalization on all the transposed convolutional layers except the last one.\n",
    "\n",
    "> **练习：**在下面的函数中构建发生器的转置卷积网络。 确保在除了最后一个tanh层之外的所有层上使用leaky ReLUs，以及除最后一个之外的所有转置的卷积层上的批归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, output_dim, reuse=False, alpha=0.2, training=True):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        # First fully connected layer\n",
    "        x\n",
    "        \n",
    "        # Output layer, 32x32x3\n",
    "        logits = \n",
    "        \n",
    "        out = tf.tanh(logits)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator 鉴别器\n",
    "\n",
    "Here you'll build the discriminator. This is basically just a convolutional classifier like you've build before. The input to the discriminator are 32x32x3 tensors/images. You'll want a few convolutional layers, then a fully connected layer for the output. As before, we want a sigmoid output, and you'll need to return the logits as well. For the depths of the convolutional layers I suggest starting with 16, 32, 64 filters in the first layer, then double the depth as you add layers. Note that in the DCGAN paper, they did all the downsampling using only strided convolutional layers with no maxpool layers.\n",
    "\n",
    "在这里，您将构建鉴别器。 这基本上只是你之前建立的卷积分类器。 鉴别器的输入为32x32x3张量/图像。 您将需要几个卷积层，然后是输出的完全连接层。 像以前一样，我们想要一个Sigmoid输出，你也需要返回逻辑。 对于卷积层的深度，我建议从第一层中的16,32,64个过滤器开始，然后在添加图层时将深度加倍。 请注意，在DCGAN论文中，他们进行了所有的采样，仅使用没有最大色散层的步进卷积层。\n",
    "\n",
    "You'll also want to use batch normalization with `tf.layers.batch_normalization` on each layer except the first convolutional and output layers. Again, each layer should look something like convolution > batch norm > leaky ReLU.\n",
    "\n",
    "您还需要在除了第一个卷积和输出层之外的每个层上使用 `tf.layers.batch_normalization` 进行批量归一化。 再次，每个层应该看起来像卷积 >批量规范 > leaky ReLU。\n",
    "\n",
    ">**Exercise:** Build the convolutional network for the discriminator. The input is a 32x32x3 images, the output is a sigmoid plus the logits. Again, use Leaky ReLU activations and batch normalization on all the layers except the first.\n",
    "\n",
    "> **练习：**构建用于鉴别器的卷积网络。 输入是一个32x32x3的图像，输出是一个sigmoid加上logits。 再次，使用Leaky ReLU激活和批次归一化除了第一层以外的所有图层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, reuse=False, alpha=0.2):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Input layer is 32x32x3\n",
    "        x =\n",
    "        \n",
    "        logits = \n",
    "        out = \n",
    "        \n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loss 模型损失\n",
    "\n",
    "Calculating the loss like before, nothing new here.\n",
    "\n",
    "计算损失像以前一样，没有什么新鲜的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, output_dim, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param input_real: Images from the real dataset\n",
    "    :param input_z: Z input\n",
    "    :param out_channel_dim: The number of channels in the output image\n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    g_model = generator(input_z, output_dim, alpha=alpha)\n",
    "    d_model_real, d_logits_real = discriminator(input_real, alpha=alpha)\n",
    "    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True, alpha=alpha)\n",
    "\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_model_real)))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_model_fake)))\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_model_fake)))\n",
    "\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers 优化器\n",
    "\n",
    "Again, nothing new here.\n",
    "\n",
    "有一个，没有什么新鲜的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, learning_rate, beta1):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator loss Tensor\n",
    "    :param g_loss: Generator loss Tensor\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "\n",
    "    # Optimize\n",
    "    d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "    g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model 建立模型\n",
    "\n",
    "Here we can use the functions we defined about to build the model as a class. This will make it easier to move the network around in our code since the nodes and operations in the graph are packaged in one object.\n",
    "\n",
    "在这里，我们可以使用我们定义的函数来将模型构建为一个类。 这将使得在我们的代码中更容易地移动网络，因为图中的节点和操作被打包在一个对象中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, real_size, z_size, learning_rate, alpha=0.2, beta1=0.5):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.input_real, self.input_z = model_inputs(real_size, z_size)\n",
    "        \n",
    "        self.d_loss, self.g_loss = model_loss(self.input_real, self.input_z,\n",
    "                                              real_size[2], alpha=0.2)\n",
    "        \n",
    "        self.d_opt, self.g_opt = model_opt(self.d_loss, self.g_loss, learning_rate, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function for displaying generated images. \n",
    "\n",
    "这是现实生成图像的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def view_samples(epoch, samples, nrows, ncols, figsize=(5,5)):\n",
    "    fig, axes = plt.subplots(figsize=figsize, nrows=nrows, ncols=ncols, \n",
    "                             sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        ax.axis('off')\n",
    "        img = ((img - img.min())*255 / (img.max() - img.min())).astype(np.uint8)\n",
    "        ax.set_adjustable('box-forced')\n",
    "        im = ax.imshow(img)\n",
    "   \n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another function we can use to train our network.\n",
    "\n",
    "另一个方法我们可以用来训练你的网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, dataset, epochs, batch_size, print_every=10, show_every=100, figsize=(5,5)):\n",
    "    saver = tf.train.Saver()\n",
    "    sample_z = np.random.uniform(-1, 1, size=(50, z_size))\n",
    "\n",
    "    samples, losses = [], []\n",
    "    steps = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for e in range(epochs):\n",
    "            for x, y in dataset.batches(batch_size):\n",
    "                steps += 1\n",
    "\n",
    "                # Sample random noise for G\n",
    "                batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "\n",
    "                # Run optimizers\n",
    "                _ = sess.run(net.d_opt, feed_dict={net.input_real: x, net.input_z: batch_z})\n",
    "                _ = sess.run(net.g_opt, feed_dict={net.input_z: batch_z})\n",
    "\n",
    "                if steps % print_every == 0:\n",
    "                    # At the end of each epoch, get the losses and print them out\n",
    "                    train_loss_d = net.d_loss.eval({net.input_z: batch_z, net.input_real: x})\n",
    "                    train_loss_g = net.g_loss.eval({net.input_z: batch_z})\n",
    "\n",
    "                    print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "                          \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "                          \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
    "                    # Save losses to view after training\n",
    "                    losses.append((train_loss_d, train_loss_g))\n",
    "\n",
    "                if steps % show_every == 0:\n",
    "                    gen_samples = sess.run(\n",
    "                                   generator(net.input_z, 3, reuse=True),\n",
    "                                   feed_dict={net.input_z: sample_z})\n",
    "                    samples.append(gen_samples)\n",
    "                    _ = view_samples(-1, samples, 5, 10, figsize=figsize)\n",
    "                    plt.show()\n",
    "\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n",
    "\n",
    "    with open('samples.pkl', 'wb') as f:\n",
    "        pkl.dump(samples, f)\n",
    "    \n",
    "    return losses, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters 超参数\n",
    "\n",
    "GANs are very senstive to hyperparameters. A lot of experimentation goes into finding the best hyperparameters such that the generator and discriminator don't overpower each other. Try out your own hyperparameters or read [the DCGAN paper](https://arxiv.org/pdf/1511.06434.pdf) to see what worked for them.\n",
    "\n",
    "GANs 对超参数非常敏感。 大量的实验发现，寻找最佳的超参数，使得发生器和鉴别器不会相互超载。 尝试您自己的超参数或阅读 [DCGAN论文](https://arxiv.org/pdf/1511.06434.pdf)，看看它们有什么用。\n",
    "\n",
    ">**Exercise:** Find hyperparameters to train this GAN. The values found in the DCGAN paper work well, or you can experiment on your own. In general, you want the discriminator loss to be around 0.3, this means it is correctly classifying images as fake or real about 50% of the time.\n",
    "\n",
    "> **练习** 查找超参数来训练这个GAN。 在DCGAN论文中找到的值很好，或者你可以自己进行实验。 一般来说，你想让鉴别器的损耗大约在0.3左右，这意味着大约50％的时间正确地将图像分类为假的或真实的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_size = (32,32,3)\n",
    "z_size = 100\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "epochs = 1\n",
    "alpha = 0.01\n",
    "beta1 = 0.9\n",
    "\n",
    "# Create the network\n",
    "net = GAN(real_size, z_size, learning_rate, alpha=alpha, beta1=beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data and train the network here\n",
    "dataset = Dataset(trainset, testset)\n",
    "losses, samples = train(net, dataset, epochs, batch_size, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = view_samples(-1, samples, 5, 10, figsize=(10,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
